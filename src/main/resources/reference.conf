kafka4m {

  # We add some ETL functionality to kafka4m for reading/writing data in and out of Kafka
  etl {

    # controls periodic read/write stats for ETL jobs
    stats {
      # if set, the ETL statistics will be periodically written to the given directory
      writeTo: ""

      # when set to true, kafka read/write stats will be tracked and periodically logged
      enabled: true

      # when stats are enablerd and the 'writeTo' is set to directory, this frequency determines how often
      # the stats reports are written to the 'writeTo' directory
      flushHtmlFrequency: 30s
    }

    # configuration for reading data into kafka
    intoKafka {
      # where the kafka data will be written to
      dataDir: ".etl-data"
      dataDir: ${?KAFKA4M_INTO_KAFKA_DIR}


      # if set to true, the file data will be read into memory.
      # this can be useful when using this for performance testing as it avoids
      # reading from disk when feeding kafka.
      # The obvious caveat is just that this assumes the data in the dataDir will
      # it into memory
      cache: false
      cache: ${?KAFKA4M_CACHE}

      # if set to a value greater than zero, then this will limite how many messages
      # per second are sent to kafka
      rateLimitPerSecond: 0
      rateLimitPerSecond: ${?KAFKA4M_RATE_LIMIT}

      # if set to a positive non-zero value, these many records will be read into kafka from
      # the data source. Handy when used in conjunction with 'repeat'
      limit: 0
      limit: ${?KAFKA4M_LIMIT}

      # useful when perf testing - if set to true the data in the dataDir will be
      # continually fed into kafka via an infinite stream
      repeat: false
      repeat: ${?KAFKA4M_REPEAT}

      # is set to true then the kafka keys will be taken from the file names. If false then the keys
      # will just be the indices of the records read in
      fileNamesAsKeys: true
    }
    # The kafka4m.io.Base64Writer configuration for
    fromKafka {
      # where the kafka data will be written to
      dataDir: ".etl-data"
      dataDir: ${?KAFKA4M_FROM_KAFKA_DIR}

      # if set to true, the dataDir will be created if it does not exist
      createDirIfMissing: true

      # the data will be written to the 'dataDir' partitioned by time.
      # this is the 'bucket size' in minutes for each partition.
      # e.g., if set to 20, then the data directory will be populated with 20 minute entries like this:
      # <dataDir>/<date>__<hour>_0-20
      # <dataDir>/<date>__<hour>_20-40
      # <dataDir>/<date>__<hour>_40-60
      timeBucketMinutes: 10

      # the number of consumer records to observe which are NOT in a particular bucket before assuming that
      # we will not see any more entries in that bucket.
      #
      # The idea is that we will be reading across partitions, and there may be some variance in the 'timestamp' for
      # each record. Once we see <recordsReceivedBeforeClosingBucket> records which are NOT in a particular time bucket,
      # that bucket will be flushed and a relevant event will be generating for the bucket/directory
      recordsReceivedBeforeClosingBucket: 100

      # The number of records appended to a file before we request the writer be flushed. This is purely a performance
      # configuration, as the writer is flushed when the bucket is closed.
      numberOfAppendsBeforeWriterFlush: 1000

      # how often to flush the output stream to disk
      flushEvery: 100

      # if set to a non-zero value then this many records will be consumed from kafka
      limit: 0
    }

  }

  # The kafka topic from which we'll read/write to. This will be used as the default (programmatically checked)
  # if the specific topic for a producer, consumer, admin or stream is not set
  topic: ""
  topic: ${?KAFKA4M_TOPIC}

  # The default boostrap server(s) to connect to
  bootstrap.servers: "localhost:9092"
  bootstrap.servers: ${?KAFKA4M_BOOTSTRAP}

  # an admin client
  admin {
    topic = ${kafka4m.topic}
    bootstrap.servers = ${kafka4m.bootstrap.servers}
    bootstrap.servers: ${?KAFKA4M_BOOTSTRAP}
  }

  # the 'producer' configuration is used to push to kafka
  producer {
    topic = ${kafka4m.topic}
    bootstrap.servers = ${kafka4m.bootstrap.servers}
    bootstrap.servers: ${?KAFKA4M_BOOTSTRAP}
    # set to true to block the observer on the completion of a write
    fireAndForget: true

    key.serializer: "org.apache.kafka.common.serialization.StringSerializer"
    value.serializer: "org.apache.kafka.common.serialization.ByteArraySerializer"
  }

  # the amount of time to wait for data to arrive from kafka
  consumer {
    topic = ${kafka4m.topic}
    bootstrap.servers = ${kafka4m.bootstrap.servers}
    bootstrap.servers: ${?KAFKA4M_BOOTSTRAP}

    max.poll.records: 1024
    max.poll.interval.ms: 5000

    # used by the KafkaConsumerFeed to determine the queue size for data pulled into the kafka consumer
    commandQueueSize: 100

    # the kafka consumer poll timeout
    pollTimeout: "200ms"

    group.id: "kafka4m-consumer"
    group.id: ${?KAFKA4M_GROUP_ID}

    auto.offset.reset: earliest
    auto.offset.reset: ${?KAFKA4M_AUTO_OFFSET}

    default.key.serde: "org.apache.kafka.common.serialization.Serdes$StringSerde"
    default.value.serde: "org.apache.kafka.common.serialization.Serdes$ByteArraySerde"
  }

  # instructs the admin client what to do when a topic is missing
  # see RichKafkaAdmin
  whenMissingTopic {
    create: true
    numPartitions: 5
    replicationFactor: 3
    timeout: 10s
  }

  # the 'streams' configuration is used for reading from Kafka
  streams {
    topic = ${kafka4m.topic}
    bootstrap.servers = ${kafka4m.bootstrap.servers}
    bootstrap.servers: ${?KAFKA4M_BOOTSTRAP}

    application.id: "kafka4m-app"
    default.key.serde: "org.apache.kafka.common.serialization.Serdes$StringSerde"
    default.value.serde: "org.apache.kafka.common.serialization.Serdes$ByteArraySerde"
    auto.offset.reset: earliest
    auto.offset.reset: ${?KAFKA4M_AUTO_OFFSET}
  }

}