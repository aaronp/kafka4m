kafka4m {

  # We add some ETL functionality to kafka4m for reading/writing data in and out of Kafka
  etl {

    # configuration for reading data into kafka
    intoKafka {
      # where the kafka data will be written to
      dataDir: ".etl-data"

      # if set to true, the file data will be read into memory.
      # this can be useful when using this for performance testing as it avoids
      # reading from disk when feeding kafka.
      # The obvious caveat is just that this assumes the data in the dataDir will
      # it into memory
      cache: false

      # if set to a value greater than zero, then this will limite how many messages
      # per second are sent to kafka
      rateLimitPerSecond: 0

      # if set to a positive non-zero value, these many records will be read into kafka from
      # the data source. Handy when used in conjunction with 'repeat'
      limit: 0

      # useful when perf testing - if set to true the data in the dataDir will be
      # continually fed into kafka via an infinite stream
      repeat: false

      # is set to true then the kafka keys will be taken from the file names. If false then the keys
      # will just be the indices of the records read in
      fileNamesAsKeys: true
    }
    # The kafka4m.io.Base64Writer configuration for
    fromKafka {
      # where the kafka data will be written to
      dataDir: ".etl-data"

      # if set to true, the dataDir will be created if it does not exist
      createDirIfMissing: true

      # the data will be written to the 'dataDir' partitioned by time.
      # this is the 'bucket size' in minutes for each partition.
      # e.g., if set to 20, then the data directory will be populated with 20 minute entries like this:
      # <dataDir>/<date>__<hour>_0-20
      # <dataDir>/<date>__<hour>_20-40
      # <dataDir>/<date>__<hour>_40-60
      timeBucketMinutes: 10

      # the number of consumer records to observe which are NOT in a particular bucket before assuming that
      # we will not see any more entries in that bucket.
      #
      # The idea is that we will be reading across partitions, and there may be some variance in the 'timestamp' for
      # each record. Once we see <recordsReceivedBeforeClosingBucket> records which are NOT in a particular time bucket,
      # that bucket will be flushed and a relevant event will be generating for the bucket/directory
      recordsReceivedBeforeClosingBucket: 100

      # The number of records appended to a file before we request the writer be flushed. This is purely a performance
      # configuration, as the writer is flushed when the bucket is closed.
      numberOfAppendsBeforeWriterFlush: 1000
    }

  }

  # The kafka topic from which we'll read/write to. This will be used as the default (programmatically checked)
  # if the specific topic for a producer, consumer, admin or stream is not set
  topic: ""

  # The default boostrap server(s) to connect to
  bootstrap.servers: "localhost:9092"

  # an admin client
  admin.topic = ${kafka4m.topic}
  admin.bootstrap.servers = ${kafka4m.bootstrap.servers}

  # the 'producer' configuration is used to push to kafka
  producer {
    topic = ${kafka4m.topic}
    bootstrap.servers = ${kafka4m.bootstrap.servers}
    # set to true to block the observer on the completion of a write
    fireAndForget: true

    key.serializer: "org.apache.kafka.common.serialization.StringSerializer"
    value.serializer: "org.apache.kafka.common.serialization.ByteArraySerializer"
  }

  # the amount of time to wait for data to arrive from kafka
  consumer {
    topic = ${kafka4m.topic}
    bootstrap.servers = ${kafka4m.bootstrap.servers}

    max.poll.records: 1024
    max.poll.interval.ms: 5000

    # used by the KafkaConsumerFeed to determine the queue size for data pulled into the kafka consumer
    commandQueueSize: 100

    pollTimeout: "200ms"
    feedTimeout: "1m"
    group.id: "kafka4m-consumer"
    auto.offset.reset: earliest
    default.key.serde: "org.apache.kafka.common.serialization.Serdes$StringSerde"
    default.value.serde: "org.apache.kafka.common.serialization.Serdes$ByteArraySerde"
  }

  # what to do when a topic is missing
  whenMissingTopic {
    create: true
    numPartitions: 5
    replicationFactor: 3
    timeout: 10s
  }

  # the 'streams' configuration is used for reading from Kafka
  streams {
    topic = ${kafka4m.topic}
    bootstrap.servers = ${kafka4m.bootstrap.servers}

    application.id: "kafka4m-app"
    default.key.serde: "org.apache.kafka.common.serialization.Serdes$StringSerde"
    default.value.serde: "org.apache.kafka.common.serialization.Serdes$ByteArraySerde"
    auto.offset.reset: earliest
  }

}