kafka4m {

  # The kafka topic from which we'll read/write to. This will be used as the default (programmatically checked)
  # if the specific topic for a producer, consumer, admin or stream is not set
  topic: ""
  topic: ${?KAFKA4M_TOPIC}

  # The default boostrap server(s) to connect to
  bootstrap.servers: "localhost:9092"
  bootstrap.servers: ${?KAFKA4M_BOOTSTRAP}

  # an admin client
  admin {
    topic = ${kafka4m.topic}
    bootstrap.servers = ${kafka4m.bootstrap.servers}
    bootstrap.servers: ${?KAFKA4M_BOOTSTRAP}
  }

  # the 'producer' configuration is used to push to kafka
  producer {
    topic = ${kafka4m.topic}
    bootstrap.servers = ${kafka4m.bootstrap.servers}
    bootstrap.servers: ${?KAFKA4M_BOOTSTRAP}
    # set to true to block the observer on the completion of a write
    fireAndForget: true

    # Should the producer simply log/continue on serialization errors?
    continueOnError: false

    # should the producer be closed when the consumer completes
    closeOnComplete : true

    key.serializer: "org.apache.kafka.common.serialization.StringSerializer"
    value.serializer: "org.apache.kafka.common.serialization.ByteArraySerializer"
  }

  # The consumer configuration as read by 'kafka4m.read(...)'
  #
  # The values specified here will be used/passed-through to the Kafka consumer when creating a
  # consumer.
  #
  consumer {
    enable.auto.commit : false

    # The kafka topic
    topic = ${kafka4m.topic}

    bootstrap.servers = ${kafka4m.bootstrap.servers}
    bootstrap.servers: ${?KAFKA4M_BOOTSTRAP}

    max.poll.records: 1024
    max.poll.interval.ms: 5000

    # the kafka consumer poll timeout
    pollTimeout: "200ms"

    group.id: "kafka4m-consumer"
    group.id: ${?KAFKA4M_GROUP_ID}

    auto.offset.reset: earliest
    auto.offset.reset: ${?KAFKA4M_AUTO_OFFSET}

    default.key.serde: "org.apache.kafka.common.serialization.Serdes$StringSerde"
    default.value.serde: "org.apache.kafka.common.serialization.Serdes$ByteArraySerde"

    # when 'asObservable' is set on a consumer, this controls whether the consumer should be closed when the
    # observable is completed.
    closeOnComplete = true

    # Used for the command queue when sending the RichKafkaConsumer commands from differnet threads.
    # Those commands should be infrequent - typically when a job run by ConcurrentStream completes and
    # fancies committing offsets. The job pool for the ConcurrentStream defaults to the number of cores,
    # so this should be a sensible default.
    #
    # If set to 0 or a negative number then this buffer will be unbounded.
    commandBufferCapacity : 100

    # should we automagically subscribe when creating a RickKafkaConsumer
    subscribeOnConnect : true
  }

  # instructs the admin client what to do when a topic is missing
  # see RichKafkaAdmin
  whenMissingTopic {
    create: true
    numPartitions: 5
    replicationFactor: 3
    timeout: 10s
  }


  jmx {
    client {
      errorWhenUnhealthy = false
      hostPort: "localhost:5555"
      hostPort: ${?JMXCLIENT_HOSTPORT}
      serviceUrl: "service:jmx:rmi:///jndi/rmi://"${kafka4m.jmx.client.hostPort}"/jmxrmi"

      reportFrequency : "10s"

      # when running the JMXClient this is the mbean to connect
      mbeanName : ""
      mbeanName : ${?JMXCLIENT_MBEAN}
    }
  }

  jobs {
    # this controls as timeout for writing offsets back to kafka.
    # in ConcurrentStream, this determines how long to wait for the last job to complete before giving up
    awaitJobTimeout : 10s

    # how long should we pause when checking for job completions status of the last job?
    # essentially a poll frequncy when checking awaitJobTimeout
    retryDuration : 100ms

    # how frequently should we commit completed task offsets back to kafka
    # set to 0 to commit as frequently as possible
    minCommitFrequency = 100
  }

  # We add some ETL functionality to kafka4m for reading/writing data in and out of Kafka
  etl {

    # controls periodic read/write stats for ETL jobs
    stats {
      # if set, the ETL statistics will be periodically written to the given directory
      writeTo: ""

      # when set to true, kafka read/write stats will be tracked and periodically logged
      enabled: true

      # when stats are enablerd and the 'writeTo' is set to directory, this frequency determines how often
      # the stats reports are written to the 'writeTo' directory
      flushHtmlFrequency: 30s
    }

    # configuration for reading data into kafka
    intoKafka {
      # where the kafka data will be written to
      dataDir: ".etl-data"
      dataDir: ${?KAFKA4M_INTO_KAFKA_DIR}


      # if set to true, the file data will be read into memory.
      # this can be useful when using this for performance testing as it avoids
      # reading from disk when feeding kafka.
      # The obvious caveat is just that this assumes the data in the dataDir will
      # it into memory
      cache: false
      cache: ${?KAFKA4M_CACHE}

      # if set to a value greater than zero, then this will limite how many messages
      # per second are sent to kafka
      rateLimitPerSecond: 0
      rateLimitPerSecond: ${?KAFKA4M_RATE_LIMIT}

      # if set to a positive non-zero value, these many records will be read into kafka from
      # the data source. Handy when used in conjunction with 'repeat'
      limit: 0
      limit: ${?KAFKA4M_LIMIT}

      # useful when perf testing - if set to true the data in the dataDir will be
      # continually fed into kafka via an infinite stream
      repeat: false
      repeat: ${?KAFKA4M_REPEAT}

      # is set to true then the kafka keys will be taken from the file names. If false then the keys
      # will just be the indices of the records read in
      fileNamesAsKeys: true
    }
    # The kafka4m.io.Base64Writer configuration for
    fromKafka {
      # where the kafka data will be written to
      dataDir: ".etl-data"
      dataDir: ${?KAFKA4M_FROM_KAFKA_DIR}

      # if set to true, the dataDir will be created if it does not exist
      createDirIfMissing: true

      # the data will be written to the 'dataDir' partitioned by time.
      # this is the 'bucket size' in minutes for each partition.
      # e.g., if set to 20, then the data directory will be populated with 20 minute entries like this:
      # <dataDir>/<date>__<hour>_0-20
      # <dataDir>/<date>__<hour>_20-40
      # <dataDir>/<date>__<hour>_40-60
      timeBucketMinutes: 10

      # the number of consumer records to observe which are NOT in a particular bucket before assuming that
      # we will not see any more entries in that bucket.
      #
      # The idea is that we will be reading across partitions, and there may be some variance in the 'timestamp' for
      # each record. Once we see <recordsReceivedBeforeClosingBucket> records which are NOT in a particular time bucket,
      # that bucket will be flushed and a relevant event will be generating for the bucket/directory
      recordsReceivedBeforeClosingBucket: 100

      # The number of records appended to a file before we request the writer be flushed. This is purely a performance
      # configuration, as the writer is flushed when the bucket is closed.
      numberOfAppendsBeforeWriterFlush: 1000

      # how often to flush the output stream to disk
      flushEvery: 100

      # if set to a non-zero value then this many records will be consumed from kafka
      limit: 0
    }

  }
}